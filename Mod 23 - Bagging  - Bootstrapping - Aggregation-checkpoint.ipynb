{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ca4c88",
   "metadata": {},
   "source": [
    "# What is Bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e3513",
   "metadata": {},
   "source": [
    "#### Saiba como a bootstrap aggregating, ou bagging, pode melhorar a precisão de seus modelos de aprendizado de máquina, permitindo que você desenvolva melhores insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf602b",
   "metadata": {},
   "source": [
    "# https://www.ibm.com/topics/bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641922d",
   "metadata": {},
   "source": [
    "### What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29a559",
   "metadata": {},
   "source": [
    "O bagging, também conhecido como aggregating de bootstrap, é o método de aprendizado de conjunto que é comumente usado para reduzir a variância dentro de um conjunto de dados barulhento. No bagging, uma amostra aleatória de dados em um conjunto de treinamento é selecionada com substituição, o que significa que os pontos de dados individuais podem ser escolhidos mais de uma vez. Depois que várias amostras de dados são geradas, esses modelos fracos são treinados de forma independente e, dependendo do tipo de tarefa – regressão ou classificação, por exemplo – a média ou a maioria dessas previsões produz uma estimativa mais precisa.\n",
    "\n",
    "Como nota, o algoritmo de floresta aleatória é considerado uma extensão do método de bagging, usando tanto o bagging quanto a aleatoriedade de recursos para criar uma floresta não correlacionada de árvores de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6db1f3",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601950d6",
   "metadata": {},
   "source": [
    "O aprendizado em conjunto dá crédito à ideia da \"sabedoria das multidões\", o que sugere que a tomada de decisão de um grupo maior de pessoas é tipicamente melhor do que a de um especialista individual. Da mesma forma, a aprendizagem em conjunto refere-se a um grupo (ou conjunto) de alunos de base, ou modelos, que trabalham coletivamente para alcançar uma melhor previsão final. Um único modelo, também conhecido como aluno básico ou fraco, pode não ter um bom desempenho individual devido à alta variância ou alto viés. No entanto, quando os alunos fracos são agregados, eles podem formar um aprendiz forte, pois sua combinação reduz o viés ou a variância, produzindo um melhor desempenho do modelo.\n",
    "\n",
    "Os métodos de conjunto são frequentemente ilustrados usando árvores de decisão, pois esse algoritmo pode ser propenso a sobreajuste (alta variância e baixo viés) quando não foi podado e também pode se prestar a subajuste (baixa variância e alto viés) quando é muito pequeno, como um toco de decisão, que é uma árvore de decisão com um nível. Lembre-se, quando um algoritmo se sobrepõe ou se ajusta ao seu conjunto de treinamento, ele não pode generalizar bem para novos conjuntos de dados, portanto, métodos de conjunto são usados para neutralizar esse comportamento para permitir a generalização do modelo para novos conjuntos de dados. Embora as árvores de decisão possam exibir alta variância ou alto viés, vale a pena notar que não é a única técnica de modelagem que aproveita o aprendizado do conjunto para encontrar o \"ponto ideal\" dentro da troca viés-variância."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5d434",
   "metadata": {},
   "source": [
    "### Bagging vs. boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e0bed",
   "metadata": {},
   "source": [
    "Bagging e boosting são dois tipos principais de métodos de aprendizagem de conjunto. Como destacado neste estudo (PDF, 248 KB) (este link reside fora de ibm.com), a principal diferença entre esses métodos de aprendizagem é a maneira como eles são treinados. No Bagging, os alunos fracos são treinados em paralelo, mas no boosting, eles aprendem sequencialmente. Isso significa que uma série de modelos é construída e, a cada nova iteração de modelo, os pesos dos dados classificados incorretamente no modelo anterior são aumentados. Essa redistribuição de pesos ajuda o algoritmo a identificar os parâmetros nos quais ele precisa se concentrar para melhorar seu desempenho. AdaBoost, que significa \"algoritmo de boosting adaptativo\", é um dos algoritmos de boosting mais populares, pois foi um dos primeiros de seu tipo. Outros tipos de algoritmos de boosting incluem XGBoost, GradientBoost e BrownBoost.\n",
    "\n",
    "Outra diferença em que o Bagging e o boosting diferem são os cenários em que são usados. Por exemplo, os métodos de Bagging são tipicamente usados em alunos fracos que exibem alta variância e baixo viés, enquanto os métodos de boosting são aproveitados quando baixa variância e alto viés são observados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9dcfdb",
   "metadata": {},
   "source": [
    "### How bagging works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f8442",
   "metadata": {},
   "source": [
    "Em 1996, Leo Breiman (PDF, 829 KB) (este link reside fora da ibm.com) introduziu o algoritmo de bagging, que tem três etapas básicas:\n",
    "\n",
    "###### Bootstrapping: O bagging aproveita uma técnica de amostragem de bootstrapping para criar amostras diversas. Esse método de reamostragem gera diferentes subconjuntos do conjunto de dados de treinamento selecionando pontos de dados aleatoriamente e com substituição. Isso significa que, sempre que você selecionar um ponto de dados do conjunto de dados de treinamento, poderá selecionar a mesma instância várias vezes. Como resultado, um valor/instância repetido duas vezes (ou mais) em um exemplo.\n",
    "###### Parallel training: Essas amostras de bootstrap são então treinadas de forma independente e em paralelo umas com as outras usando alunos fracos ou básicos.\n",
    "###### Aggregation: Finalmente, dependendo da tarefa (ou seja, regressão ou classificação), uma média ou a maioria das previsões são tomadas para calcular uma estimativa mais precisa. No caso de regressão, toma-se uma média de todas as saídas previstas pelos classificadores individuais; isso é conhecido como voto suave. Para problemas de classificação, a classe com a maior maioria de votos é aceita; isso é conhecido como voto duro ou voto por maioria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3650c5c",
   "metadata": {},
   "source": [
    "### Benefits and challenges of bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138aecd",
   "metadata": {},
   "source": [
    "Há uma série de vantagens e desafios importantes que o método de bagging apresenta quando usado para problemas de classificação ou regressão. Os principais benefícios do bagging incluem:\n",
    "\n",
    "Facilidade de implementação: Bibliotecas Python como scikit-learn (também conhecido como sklearn) facilitam a combinação das previsões de alunos de base ou estimadores para melhorar o desempenho do modelo. Sua documentação (link reside fora da IBM) estabelece os módulos disponíveis que você pode aproveitar em sua otimização de modelo.\n",
    "Redução da variância: O bagging pode reduzir a variância dentro de um algoritmo de aprendizagem. Isso é particularmente útil com dados de alta dimensão, onde os valores ausentes podem levar a uma maior variância, tornando-os mais propensos ao excesso de ajuste e impedindo a generalização precisa para novos conjuntos de dados.\n",
    "Os principais desafios do bagging incluem:\n",
    "\n",
    "Perda de interpretabilidade: É difícil extrair insights de negócios muito precisos por meio do bagging, devido à média envolvida nas previsões. Embora a saída seja mais precisa do que qualquer ponto de dados individual, um conjunto de dados mais preciso ou completo também pode produzir mais precisão dentro de um único modelo de classificação ou regressão.\n",
    "Computacionalmente caro: O bagging diminui e se torna mais intenso à medida que o número de iterações aumenta. Assim, não é adequado para aplicações em tempo real. Sistemas em cluster ou um grande número de núcleos de processamento são ideais para criar rapidamente conjuntos bagging em grandes conjuntos de teste.\n",
    "Menos flexível: Como técnica, o bagging funciona particularmente bem com algoritmos que são menos estáveis. Um que é mais estável ou sujeito a altas quantidades de viés não fornece tanto benefício, pois há menos variação dentro do conjunto de dados do modelo. Conforme observado no Hands-On Guide to Machine Learning (link reside fora da IBM), \"bagging um modelo de regressão linear efetivamente retornará as previsões originais para b grande o suficiente\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fafd5",
   "metadata": {},
   "source": [
    "### Aplicações de bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca8ada",
   "metadata": {},
   "source": [
    "A técnica de bagging é usada em um grande número de indústrias, fornecendo insights para o valor do mundo real e perspectivas interessantes, como nos Debates do GRAMMY com Watson. Os principais casos de uso incluem:\n",
    "\n",
    "##### Saúde: O bagging tem sido usado para formar previsões de dados médicos. Por exemplo, a pesquisa (PDF, 2,8 MB) (este link reside fora de ibm.com) mostra que os métodos de conjunto têm sido usados para uma série de problemas de bioinformática, como a seleção de genes e / ou proteínas para identificar uma característica específica de interesse. Mais especificamente, esta pesquisa (este link reside fora de ibm.com) investiga seu uso para prever o início do diabetes com base em vários preditores de risco.\n",
    "##### IT: O bagging também pode melhorar a precisão e a exatidão em sistemas de TI, como os sistemas de detecção de intrusão de rede. Enquanto isso, esta pesquisa (este link reside fora de ibm.com) analisa como o bagging pode melhorar a precisão da detecção de intrusão de rede e reduzir as taxas de falsos positivos.\n",
    "##### Ambiente: Métodos de conjunto, como bagging, foram aplicados no campo do sensoriamento remoto. Mais especificamente, esta pesquisa (este link reside fora de ibm.com) mostra como ele tem sido usado para mapear os tipos de zonas úmidas dentro de uma paisagem costeira.\n",
    "##### Finanças: O Bagging também foi alavancado com modelos de aprendizado profundo no setor financeiro, automatizando tarefas críticas, incluindo detecção de fraudes, avaliações de risco de crédito e problemas de precificação de opções. Esta pesquisa (este link reside fora de ibm.com) demonstra como o bagging entre outras técnicas de aprendizado de máquina foi aproveitado para avaliar o risco de inadimplência do empréstimo. Este estudo (este link reside fora da ibm.com) destaca como o bagging ajuda a minimizar o risco, evitando fraudes com cartão de crédito dentro de instituições bancárias e financeiras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856846dd",
   "metadata": {},
   "source": [
    "# Aqui está um exemplo de como implementar o Bagging usando Python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a748d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a toy classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base estimator\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Define the bagging classifier with 10 base estimators\n",
    "bagging = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the bagging classifier on the training data\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the bagging classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64d163",
   "metadata": {},
   "source": [
    "Neste exemplo, usamos a função do módulo para gerar um conjunto de dados de classificação de brinquedos. Em seguida, dividimos os dados em conjuntos de treinamento e teste usando a função. 'make_classification' 'sklearn.datasets' 'train_test_split'\n",
    "\n",
    "Em seguida, definimos o estimador de base como um e o classificador de ensacamento como um com 10 estimadores de base. Em seguida, ajustamos o classificador de ensacamento aos dados de treinamento e fazemos previsões sobre os dados de teste usando o método. 'DecisionTreeClassifier' 'BaggingClassifier' 'predict'\n",
    "\n",
    "Finalmente, avaliamos a precisão do classificador de ensacamento usando a função do módulo. 'accuracy_score' 'sklearn.metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b3407",
   "metadata": {},
   "source": [
    "## agregação de bootstrap de Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3edeb2",
   "metadata": {},
   "source": [
    "Bagging, abreviação de Bootstrap Aggregating, é um método de conjunto de aprendizado de máquina que envolve o treinamento de vários modelos em diferentes subconjuntos dos dados de treinamento e a combinação de suas previsões para fazer uma previsão final. A ideia por trás do Bagging é reduzir a variância de um modelo, introduzindo aleatoriedade no processo de treinamento.\n",
    "\n",
    "A parte de bootstrap do Bagging refere-se ao fato de que cada subconjunto dos dados de treinamento é criado por amostragem aleatória com substituição. Isso significa que algumas observações podem ser repetidas em um determinado subconjunto e algumas podem ser deixadas de fora. Ao amostrar repetidamente com substituição, um novo conjunto de dados de treinamento pode ser gerado para cada modelo no conjunto.\n",
    "\n",
    "Uma vez que os modelos são treinados em seus respectivos subconjuntos de dados de treinamento, suas previsões são combinadas usando um método de média simples para problemas de regressão ou um método de votação para problemas de classificação. A previsão final é muitas vezes mais precisa e menos propensa a overfitting do que a de um único modelo treinado em todo o conjunto de dados.\n",
    "\n",
    "O Bagging é um método de conjunto popular e é usado em muitas aplicações, incluindo árvores de decisão, redes neurais e máquinas vetoriais de suporte. É uma ferramenta poderosa para melhorar a precisão e a estabilidade dos modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937c059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
